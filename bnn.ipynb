{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.optim import Adam\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loaders for MNIST dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=256, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro based linear layer\n",
    "class PyroLinear(PyroModule):\n",
    "    def __init__(self, in_feat, out_feat, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_feat, out_feat, bias=True)\n",
    "\n",
    "        weight_loc = torch.tensor([0.])\n",
    "        weight_scale = torch.tensor([1.])\n",
    "        bias_loc = torch.tensor([0.])\n",
    "        bias_scale = torch.tensor([1.])\n",
    "        if use_cuda:\n",
    "            weight_loc = weight_loc.cuda()\n",
    "            weight_scale = weight_scale.cuda()\n",
    "            bias_loc = bias_loc.cuda()\n",
    "            bias_scale = bias_scale.cuda()\n",
    "\n",
    "        self.linear.weight = PyroSample(dist.Normal(weight_loc, weight_scale).expand([out_feat, in_feat]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(bias_loc, bias_scale).expand([out_feat]).to_event(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Pyro based bayesian neural net\n",
    "class BNN(PyroModule):\n",
    "    def __init__(self, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.fc1 = PyroLinear(28 * 28, 1024, use_cuda=use_cuda)\n",
    "        self.fc2 = PyroLinear(1024, 10, use_cuda=use_cuda)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 784)\n",
    "        out = self.fc2(self.fc1(x))\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        with pyro.plate('data', x.shape[0], device=x.device):\n",
    "            preds = pyro.sample('obs', dist.Categorical(logits=logits).to_event(1), obs=y)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1 epoch\n",
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x, y)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "\n",
    "# Evaluate the current model on test set\n",
    "def evaluate(model, guide, svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    total = 0.\n",
    "    correct = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, y in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x, y)\n",
    "        \n",
    "        # draw samples from posterior predictive\n",
    "        # to compute current accuracy\n",
    "        with torch.no_grad():\n",
    "            post_pred = Predictive(model, guide=guide,\n",
    "                                   num_samples=10,\n",
    "                                   return_sites=('obs', '_RETURN'))\n",
    "            pred_samples = post_pred(x)\n",
    "            pred_mean = torch.mean(pred_samples['_RETURN'], dim=0)\n",
    "            total += y.size(0)\n",
    "            correct += (pred_mean.argmax(dim=-1) == y).sum().item()\n",
    "    acc = (correct / total) * 100\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "TEST_FREQUENCY = 2\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "model = BNN(use_cuda=USE_CUDA)\n",
    "guide = AutoDiagonalNormal(model)\n",
    "adam = Adam({'lr': 3e-4})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 8523.7005\n",
      "[epoch 000] average test loss: 7204.7033 test acc: 84.44\n",
      "[epoch 001]  average training loss: 6965.4271\n",
      "[epoch 002]  average training loss: 6679.5917\n",
      "[epoch 002] average test loss: 6723.2697 test acc: 88.98\n",
      "[epoch 003]  average training loss: 6545.3954\n",
      "[epoch 004]  average training loss: 6425.4658\n",
      "[epoch 004] average test loss: 6497.1984 test acc: 89.98\n",
      "[epoch 005]  average training loss: 6304.9233\n",
      "[epoch 006]  average training loss: 6226.8963\n",
      "[epoch 006] average test loss: 6293.9690 test acc: 91.04\n",
      "[epoch 007]  average training loss: 6119.6426\n",
      "[epoch 008]  average training loss: 6034.7155\n",
      "[epoch 008] average test loss: 6103.0905 test acc: 91.63\n",
      "[epoch 009]  average training loss: 5950.8363\n",
      "[epoch 010]  average training loss: 5862.6224\n",
      "[epoch 010] average test loss: 5937.2643 test acc: 91.73\n",
      "[epoch 011]  average training loss: 5780.4665\n",
      "[epoch 012]  average training loss: 5690.7411\n",
      "[epoch 012] average test loss: 5760.9924 test acc: 92.04\n",
      "[epoch 013]  average training loss: 5623.2550\n",
      "[epoch 014]  average training loss: 5555.4575\n",
      "[epoch 014] average test loss: 5660.6889 test acc: 91.82\n",
      "[epoch 015]  average training loss: 5471.4569\n",
      "[epoch 016]  average training loss: 5425.2979\n",
      "[epoch 016] average test loss: 5482.1797 test acc: 92.05\n",
      "[epoch 017]  average training loss: 5324.0713\n",
      "[epoch 018]  average training loss: 5257.3347\n",
      "[epoch 018] average test loss: 5340.5806 test acc: 92.04\n",
      "[epoch 019]  average training loss: 5186.4653\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test, acc = evaluate(model, guide, svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f test acc: %.2f\" % (epoch, total_epoch_loss_test, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #plt.imshow(npimg,  cmap='gray')\n",
    "    #fig.show(figsize=(1,1))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(image[21].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred = Predictive(model, guide=guide,\n",
    "                       num_samples=10,\n",
    "                       return_sites=('obs', '_RETURN'))\n",
    "pred_samples = post_pred(image[21].cuda())\n",
    "pred_mean = torch.mean(pred_samples['_RETURN'], dim=0)\n",
    "pred_std = torch.std(pred_samples['_RETURN'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred_mean, dim=-1).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -40.6440, -138.0764,  -80.9457,  -59.9982, -104.4311,  -18.4773,\n",
       "          -17.3751, -175.5134,  -23.2603,  -71.6453]], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.3207, 11.6831, 21.6759, 14.1366, 17.6876, 19.6857, 19.2362, 23.1347,\n",
       "         12.8538, 17.2100]], device='cuda:0', grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
